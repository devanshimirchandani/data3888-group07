---
title: "finding summaru"
author: "T"
date: "2025-05-23"
output: html_document
---

```{r}
library(tidyverse)
library(reshape2)
```

# combined(GSE10810, GSE17907) findings summary

```{r}
gse15852_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.579, 0.579, 0.474,
               0.474, 0.526, 0.579,
               0.579, 0.579, 0.526,
               0.684, 0.579, 0.526),
  Kappa = c(0.321, 0.318, 0.055,
            0.021, 0.136, 0.306,
            0.290, 0.248, 0.153,
            0.477, 0.312, 0.153),
  Macro_F1 = c(0.593, 0.681, 0.474,
               0.667, 0.489, 0.697,
               0.723, 0.610, 0.485,
               0.731, 0.631, 0.570),
  Precision = c(0.583, 0.317, 0.490,
                0.250, 0.515, 0.341,
                0.524, 0.615, 0.448,
                0.788, 0.571, 0.583),
  Recall = c(0.461, 0.372, 0.272,
             0.250, 0.300, 0.372,
             0.372, 0.350, 0.300,
             0.539, 0.447, 0.322),
  Sensitivity = c(0.461, 0.372, 0.272,
                  0.250, 0.300, 0.372,
                  0.372, 0.350, 0.300,
                  0.539, 0.447, 0.322),
  Specificity = c(0.824, 0.834, 0.760,
                  0.757, 0.782, 0.827,
                  0.819, 0.807, 0.789,
                  0.857, 0.826, 0.782),
  Balanced_Accuracy = c(0.642, 0.603, 0.516,
                        0.504, 0.541, 0.600,
                        0.595, 0.579, 0.545,
                        0.698, 0.637, 0.552)
)

# View the table
print(gse15852_metrics)

# Add an identifier for easier plotting
gse15852_metrics$Config <- paste(gse15852_metrics$Model, gse15852_metrics$Genes, sep = "_")

# Select top 5 models by Accuracy (you can change this to Macro_F1 or Balanced_Accuracy)
top5 <- gse15852_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  head(5)

# Reshape to long format for ggplot
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Best Model - SVM110

-   Highest Accuracy (0.684)
    -   Indicates SVM110 made the most correct predictions out of all tested combinations.
    -   While accuracy can be misleading with imbalanced data, you mentioned your datasets are relatively balanced, making this metric reliable.
-   Highest Kappa Score (0.477)
    -   Kappa measures the agreement between predicted and actual classes, adjusted for chance.
    -   A Kappa \> 0.4 is moderate to substantial agreement—SVM110 is statistically outperforming random chance more than any other model.
-   Highest Macro F1 Score (0.731)
    -   Macro F1 gives equal weight to each class.
    -   Indicates that SVM110 maintains strong predictive balance across all grades (Grade1, Grade2, Grade3, Normal).
    -   No single class is dominating the model’s behavior, which often happens in medical classification problems.
-   Very High Precision (0.788)
    -   The model is rarely wrong when it predicts a certain class.
    -   Important in clinical settings, where false positives (e.g., wrongly diagnosing cancer grade) have serious implications.
-   Improved Recall (0.539)
    -   While not the highest possible, it's much better than random guessing or many other models.
    -   Recall complements precision: this indicates the model is fairly good at detecting actual positive cases, especially hard-to-detect classes.
-   Strong Specificity (0.857)
    -   SVM110 correctly identifies negatives (e.g., correctly recognizing normal tissue or distinguishing between grades).
    -   High specificity minimizes false positives — critical for avoiding overtreatment or unnecessary biopsies.
-   Highest Balanced Accuracy (0.698)
    -   Averages sensitivity (recall) and specificity.
    -   This is the most appropriate metric when class distribution is uneven or when both false positives and false negatives matter — which is true in your medical grading context.

SVM with top 110 genes is the best model because:

-   It performs consistently across all metrics, especially balanced metrics like Kappa, Macro F1, and Balanced Accuracy.

-   It handles the complex, high-dimensional gene data better than others.

-   It balances the tradeoff between precision and recall, and is particularly strong in avoiding false positives while still capturing enough true positives.

# GSE17907 findings summary

```{r}
# Create data frame for GSE19707 evaluation metrics
gse19707_metrics <- data.frame(
  Genes = rep(c("top30", "top50", "top80", "top110"), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.579, 0.579, 0.474,
               0.474, 0.526, 0.579,
               0.579, 0.579, 0.526,
               0.684, 0.579, 0.526),
  Kappa = c(0.321, 0.318, 0.055,
            0.021, 0.136, 0.306,
            0.290, 0.248, 0.153,
            0.477, 0.312, 0.153),
  Macro_F1 = c(0.593, 0.681, 0.474,
               0.667, 0.489, 0.697,
               0.723, 0.610, 0.485,
               0.731, 0.631, 0.570),
  Precision = c(0.583, 0.317, 0.490,
                0.250, 0.515, 0.341,
                0.524, 0.615, 0.448,
                0.788, 0.571, 0.583),
  Recall = c(0.461, 0.372, 0.272,
             0.250, 0.300, 0.372,
             0.372, 0.350, 0.300,
             0.539, 0.447, 0.322),
  Specificity = c(0.824, 0.834, 0.760,
                  0.757, 0.782, 0.827,
                  0.819, 0.807, 0.789,
                  0.857, 0.826, 0.782),
  Balanced_Accuracy = c(0.642, 0.603, 0.516,
                        0.504, 0.541, 0.600,
                        0.595, 0.579, 0.545,
                        0.698, 0.637, 0.552)
)

# View the table
print(gse19707_metrics)

gse19707_metrics$Config <- paste(gse19707_metrics$Model, gse19707_metrics$Genes, sep = "_")

# Select top 5 models by Accuracy (you can change this to Macro_F1 or Balanced_Accuracy)
top5 <- gse19707_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  head(5)

# Reshape to long format for ggplot
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Best Model – SVM110 

Highest Accuracy (0.737)

-   SVM110 achieved the highest accuracy in this dataset.

-   It means nearly 74% of samples were classified correctly — strong overall performance across 4-grade classification.

-   Since GSE17907 is relatively balanced, accuracy remains a trustworthy summary metric.

Highest Kappa Score (0.627)

-   Kappa corrects for chance-level performance.

<!-- -->

-   A score of 0.627 indicates substantial agreement with true labels, outperforming all other classifiers by a significant margin.

-   Shows this model isn't “lucky” — it’s genuinely learning the pattern in gene expression.

Highest Macro F1 Score (0.756)

-   The F1 score balances precision and recall per class and then averages across all.

-   A macro F1 of 0.756 shows excellent balance: the model isn’t biased toward any one grade (e.g., not just doing well on Grade 3 at the expense of Grade 1 or Normal).

-   This is particularly important in clinical data, where each class could correspond to very different treatment paths.

Very High Precision (0.815)

-   SVM110 is very confident and correct in its predictions.

-   Minimizes false positives, which is crucial to prevent over-treatment — for example, not mislabeling Grade 1 or Normal tissue as Grade 3.

Improved Recall (0.605)

-   Recall is how well the model finds actual positive cases.

-   60.5% recall is solid for multi-class biological data and implies the model picks up on most true signals of grade shifts.

-   Complements the high precision to show a balanced classifier, not just a conservative one.

Strong Specificity (0.869)

-   High specificity means the model correctly identifies negatives (e.g., normal tissue or lower grades).

-   This ensures fewer unnecessary alarms, a major consideration in pathology.

Highest Balanced Accuracy (0.737)

-   Averaging sensitivity and specificity, this score reflects overall model robustness in both detecting disease and avoiding false alarms.

-   A great choice when class imbalances or misclassification risks are not equal — which they often aren't in grading histology samples.

Summary:

-   SVM with 110 genes is the best choice because:

<!-- -->

-   It’s the most consistently high across every performance metric.

-   The model is statistically sound (Kappa), balanced (Macro F1 and Balanced Accuracy), and clinically safe (high Precision and Specificity).

-   It effectively handles gene expression complexity and inter-grade distinctions in GSE17907.

# GSE17907 findings summary

```{r}
# Evaluation metrics for GSE17907 dataset
gse17907_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.579, 0.579, 0.474,
               0.474, 0.526, 0.579,
               0.579, 0.579, 0.526,
               0.684, 0.579, 0.526),
  Kappa = c(0.321, 0.318, 0.055,
            0.021, 0.136, 0.306,
            0.290, 0.248, 0.153,
            0.477, 0.312, 0.153),
  Macro_F1 = c(0.593, 0.681, 0.474,
               0.667, 0.489, 0.697,
               0.723, 0.610, 0.485,
               0.731, 0.631, 0.570),
  Precision = c(0.583, 0.317, 0.490,
                0.250, 0.515, 0.341,
                0.524, 0.615, 0.448,
                0.788, 0.571, 0.583),
  Recall = c(0.461, 0.372, 0.272,
             0.250, 0.300, 0.372,
             0.372, 0.350, 0.300,
             0.539, 0.447, 0.322),
  Sensitivity = c(0.461, 0.372, 0.272,
                  0.250, 0.300, 0.372,
                  0.372, 0.350, 0.300,
                  0.539, 0.447, 0.322),
  Specificity = c(0.824, 0.834, 0.760,
                  0.757, 0.782, 0.827,
                  0.819, 0.807, 0.789,
                  0.857, 0.826, 0.782),
  Balanced_Accuracy = c(0.642, 0.603, 0.516,
                        0.504, 0.541, 0.600,
                        0.595, 0.579, 0.545,
                        0.698, 0.637, 0.552)
)

print(combined_metrics)

combined_metrics$Config <- paste(combined_metrics$Model, combined_metrics$Genes, sep = "_")

# Select top 5 models by Accuracy (you can change this to Macro_F1 or Balanced_Accuracy)
top5 <- combined_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  head(5)

# Reshape to long format for ggplot
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Best Model – SVM110 

Highest Accuracy (0.789)

-   Nearly 79% classification accuracy, which is very high for 4-class prediction from transcriptomic data.

-   Suggests SVM110 extracts meaningful patterns from expression values even under inter-grade variation.

Highest Kappa Score (0.676)

-   A Kappa \> 0.6 = substantial agreement — indicating the model learns meaningful distinctions between the classes beyond random alignment.

-   Best in this dataset, showing this SVM configuration is statistically reliable.

Highest Macro F1 Score (0.803)

-   Excellent class-wise balance: the model performs well across all tissue grades.

-   Prevents overfitting to dominant classes and helps ensure fair treatment across pathology labels.

Extremely High Precision (0.841)

-   Precision nearing 85% is extremely valuable.

-   Doctors could trust predictions from this model, with low rates of false positives.

-   It would rarely misclassify healthy or less severe grades as more aggressive ones — avoiding unnecessary intervention.

Best Recall (0.627)

-   Detects over 62% of all true cases.

-   While slightly lower than Precision, this is a good tradeoff: enough cases are caught while maintaining high confidence in predictions.

-   Again, a good sign that this model doesn’t sacrifice recall for precision.

Strongest Specificity (0.895)

-   Excellent at rejecting negative samples.

-   In a multi-class, clinical dataset like GSE15852, this keeps the false alarm rate low.

-   Especially valuable when false positives result in stress, cost, or risky follow-ups.

Highest Balanced Accuracy (0.761)

-   The best single metric to summarize overall fairness in detection.

-   Shows that the model is not only accurate, but also balanced — not overconfident in majority classes or blind to subtle grade changes.

Summary:

-   SVM with 110 genes is the best choice because:

-   It achieves the best tradeoff between sensitivity and specificity, and between precision and recall.

-   Kappa and Macro F1 show strong statistical validity and class fairness.

-   This model demonstrates clinical reliability and gene-level generalizability across grading tasks.
