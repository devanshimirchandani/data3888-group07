---
title: "finding summaru"
author: "T"
date: "2025-05-23"
output: html_document
---

```{r}
library(tidyverse)
library(reshape2)
```

# combined(GSE10810, GSE17907) findings summary

```{r}
library(dplyr)
library(reshape2)
library(ggplot2)

combine_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.667, 0.571, 0.714,
               0.667, 0.571, 0.667,
               0.571, 0.571, 0.714,
               0.714, 0.571, 0.714),
  Kappa = c(0.46, 0.426, 0.568,
            0.46, 0.408, 0.495,
            0.295, 0.392, 0.55,
            0.556, 0.408, 0.538),
  Macro_F1 = c(0.809, 0.667, 0.69,
               0.809, 0.633, 0.629,
               0.684, 0.624, 0.648,
               0.678, 0.633, 0.848),
  Precision = c(0.524, 0.567, 0.55,
                0.524, 0.643, 0.469,
                0.5, 0.625, 0.659,
                0.534, 0.643, 0.514),
  Recall = c(0.437, 0.458, 0.507,
             0.437, 0.424, 0.479,
             0.365, 0.424, 0.507,
             0.507, 0.424, 0.472),
  Sensitivity = c(0.437, 0.458, 0.507,
                  0.437, 0.424, 0.479,
                  0.365, 0.424, 0.507,
                  0.507, 0.424, 0.472),
  Specificity = c(0.863, 0.869, 0.898,
                  0.863, 0.863, 0.878,
                  0.821, 0.854, 0.887,
                  0.889, 0.863, 0.884),
  Balanced_Accuracy = c(0.65, 0.663, 0.702,
                        0.65, 0.643, 0.678,
                        0.593, 0.639, 0.697,
                        0.698, 0.643, 0.678)
)

# View the table
print(combine_metrics)

# Add an identifier for easier plotting
combine_metrics$Config <- paste(combine_metrics$Model, combine_metrics$Genes, sep = "_")

# Select top 5 models by Balanced Accuracy
top5 <- combine_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  head(5)

# Reshape to long format for ggplot
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Best Model – RF110 (Random Forest with Top 110 Genes) 

**Best Macro F1 Score (0.848)**

-   This means RF110 does well across all classes (Normal, Grade1, Grade2, Grade3), not just one.

-   It’s balanced — each class is treated fairly.

**High Accuracy (0.714)**

-   RF110 made the most correct predictions overall (tied with other top models).

-   Since your data is balanced, accuracy is a reliable measure.

**Strong Balanced Accuracy (0.678)**

-   Balanced Accuracy averages how well the model detects true positives and avoids false positives.

-   RF110 is consistent and fair in both areas — important in medical decisions.

**High Kappa Score (0.538)**

-   This means RF110’s predictions agree with the actual labels much better than random guessing.

**Good Precision (0.514)**

-   When RF110 predicts a class, it’s often correct.

-   Helps reduce false positives (like wrongly diagnosing a severe grade).

**Good Recall (0.472)**

-   RF110 can catch actual positive cases — important so serious grades aren’t missed.

**High Specificity (0.884)**

-   It’s very good at correctly identifying negatives (e.g., normal tissue).

-   Helps avoid unnecessary treatments or procedures.

**Why RF110 is the Best Overall:**

-   Best overall balance across all metrics.

<!-- -->

-   It’s strong at both detecting real cases and avoiding false alarms.

-   Performs especially well in a medical setting where both precision and recall are critical.

# GSE17907 findings summary

```{r}
library(ggplot2)
library(reshape2)
library(dplyr)

# Evaluation metrics for GSE17907 dataset
gse17907_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.5, 0.667, 0.583,
               0.583, 0.667, 0.583,
               0.667, 0.667, 0.583,
               0.583, 0.583, 0.583),
  Kappa = c(-0.029, 0.36, 0.143,
            0, 0.36, 0.143,
            0.262, 0.36, 0.143,
            0, 0.143, 0.143),
  Macro_F1 = c(0.706, 0.639, 0.686,
               0.737, 0.639, 0.686,
               0.722, 0.639, 0.686,
               0.737, 0.686, 0.686),
  Precision = c(0.3, 0.722, 0.533,
                0.583, 0.722, 0.533,
                0.818, 0.722, 0.533,
                0.583, 0.533, 0.533),
  Recall = c(0.214, 0.464, 0.339,
             0.25, 0.464, 0.339,
             0.375, 0.464, 0.339,
             0.25, 0.339, 0.339),
  Sensitivity = c(0.214, 0.464, 0.339,
                  0.25, 0.464, 0.339,
                  0.375, 0.464, 0.339,
                  0.25, 0.339, 0.339),
  Specificity = c(0.75, 0.825, 0.775,
                  0.75, 0.825, 0.775,
                  0.8, 0.825, 0.775,
                  0.75, 0.775, 0.775),
  Balanced_Accuracy = c(0.482, 0.645, 0.557,
                        0.5, 0.645, 0.557,
                        0.588, 0.645, 0.557,
                        0.5, 0.557, 0.557)
)

# Add unique Config column
gse17907_metrics$Config <- paste(gse17907_metrics$Model, gse17907_metrics$Genes, sep = "_")

# Remove duplicated rows just in case (optional but safe)
gse17907_metrics <- distinct(gse17907_metrics)

# Select top 5 *distinct* models by Balanced Accuracy
top5 <- gse17907_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  slice_head(n = 5)

# Reshape to long format for heatmap
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Best Model – KNN (Top30/50/80 genes) 

**Strong Accuracy (0.667)**

-   Makes about 2 out of 3 predictions correctly.

<!-- -->

-   Matches the top performers in this dataset.

**Highest Kappa (0.360)**

-   KNN predictions agree with true labels much more than random chance.

**Good Macro F1 Score (0.639)**

-   Indicates balanced performance across all grades.

<!-- -->

-   No single class is dominating the results — important for fairness in medical data.

**High Precision (0.722)**

-   When KNN predicts a specific grade, it's usually right.

<!-- -->

-   Helps avoid false positives, like overdiagnosing a cancer grade.

**Reasonable Recall (0.464)**

-   KNN finds about 46% of all actual positive cases.

<!-- -->

-   It may miss some positives, but it outperforms other models here.

**Highest Specificity (0.825)**

-   Great at identifying true negatives — important to avoid unnecessary treatment.

**Best Balanced Accuracy (0.645)**

-   Combines recall and specificity — a balanced view.

-   Shows the model is not biased toward only positive or negative predictions.

**Why KNN is Best for GSE17907**

-   It outperforms SVM and RF on every metric except F1 (which SVM slightly improves).

<!-- -->

-   Delivers consistent results across multiple top gene sets.

-   Especially strong in precision, specificity, and overall balanced performance — key for medical diagnostics.

# GSE1582 findings summary

```{r}
library(ggplot2)
library(reshape2)
library(dplyr)

# Evaluation metrics from confusion matrices
gse17907_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.579, 0.579, 0.474,
               0.474, 0.526, 0.579,
               0.579, 0.579, 0.526,
               0.684, 0.579, 0.526),
  Kappa = c(0.321, 0.318, 0.055,
            0.021, 0.136, 0.306,
            0.290, 0.248, 0.153,
            0.477, 0.312, 0.153),
  Macro_F1 = c(0.593, 0.681, 0.474,
               0.667, 0.489, 0.697,
               0.723, 0.610, 0.485,
               0.731, 0.631, 0.570),
  Precision = c(0.583, 0.317, 0.490,
                0.250, 0.515, 0.341,
                0.524, 0.615, 0.448,
                0.788, 0.571, 0.583),
  Recall = c(0.461, 0.372, 0.272,
             0.250, 0.300, 0.372,
             0.372, 0.350, 0.300,
             0.539, 0.447, 0.322),
  Sensitivity = c(0.461, 0.372, 0.272,
                  0.250, 0.300, 0.372,
                  0.372, 0.350, 0.300,
                  0.539, 0.447, 0.322),
  Specificity = c(0.824, 0.834, 0.760,
                  0.757, 0.782, 0.827,
                  0.819, 0.807, 0.789,
                  0.857, 0.826, 0.782),
  Balanced_Accuracy = c(0.642, 0.603, 0.516,
                        0.504, 0.541, 0.600,
                        0.595, 0.579, 0.545,
                        0.698, 0.637, 0.552)
)

# Add Config column
gse17907_metrics$Config <- paste(gse17907_metrics$Model, gse17907_metrics$Genes, sep = "_")

# Select top 5 models by Balanced Accuracy
top5 <- gse17907_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  head(5)

# Reshape to long format for heatmap
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "plasma", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

# Best Model – SVM (Top110 genes)

**Strong Accuracy (0.684)**

-   Correctly classifies about 68% of samples, outperforming others in this dataset.

**Highest Kappa (0.477)**

-   Shows substantial agreement between predictions and true labels beyond random chance, indicating reliable classification.

**Good Macro F1 Score (0.731)**

-   Balances precision and recall well across all grades, suggesting consistent performance on each class.

**High Precision (0.788)**

-   When SVM predicts a specific grade, it is usually correct, reducing the risk of false positives, which is crucial in medical diagnosis.

**Reasonable Recall (0.539)**

-   Finds about 54% of all actual positive cases, a good balance that helps detect many true positives while avoiding excessive false alarms.

**Highest Specificity (0.857)**

-   Excellent at correctly identifying true negatives, helping to prevent unnecessary treatments or interventions.

**Best Balanced Accuracy (0.698)**

-   Combines recall and specificity effectively, showing the model is well-balanced and unbiased toward any class.

**Why SVM is Best for GSE15852**

-   Outperforms KNN and RF on most metrics, especially precision, specificity, and overall balanced accuracy.

-   Consistently strong across the largest gene set tested, indicating stability and robustness.

-   Its balance between sensitivity and specificity is ideal for medical diagnostics where both false negatives and false positives carry serious consequences.
