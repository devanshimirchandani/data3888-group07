---
title: "finding summaru"
author: "T"
date: "2025-05-23"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r}
library(tidyverse)
library(reshape2)
```

# combined(GSE10810, GSE17907) findings summary

```{r}
library(dplyr)
library(reshape2)
library(ggplot2)

combine_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.619, 0.524, 0.571,
               0.571, 0.524, 0.619,
               0.619, 0.571, 0.619,
               0.667, 0.524, 0.571),
  Kappa = c(0.408, 0.281, 0.368,
            0.325, 0.314, 0.425,
            0.378, 0.372, 0.447,
            0.481, 0.367, 0.366),
  Macro_F1 = c(0.75, 0.663, 0.558,
               0.703, 0.706, 0.607,
               0.75, 0.777, 0.642,
               0.654, 0.587, 0.583),
  Precision = c(0.462, 0.417, 0.427,
                0.457, 0.471, 0.489,
                0.511, 0.4, 0.491,
                0.654, 0.539, 0.435),
  Recall = c(0.409, 0.353, 0.416,
             0.373, 0.353, 0.443,
             0.401, 0.381, 0.478,
             0.463, 0.388, 0.45),
  Sensitivity = c(0.409, 0.353, 0.416,
                  0.373, 0.353, 0.443,
                  0.401, 0.381, 0.478,
                  0.463, 0.388, 0.45),
  Specificity = c(0.856, 0.823, 0.847,
                  0.832, 0.836, 0.859,
                  0.842, 0.852, 0.868,
                  0.871, 0.859, 0.842),
  Balanced_Accuracy = c(0.632, 0.588, 0.632,
                        0.603, 0.595, 0.651,
                        0.621, 0.616, 0.673,
                        0.667, 0.624, 0.646)
)



# View the table
print(combine_metrics)

# Add an identifier for easier plotting
combine_metrics$Config <- paste(combine_metrics$Model, combine_metrics$Genes, sep = "_")

# Select top 5 models by Balanced Accuracy
top5 <- combine_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  head(5)

# Reshape to long format for ggplot
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "#d07095", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Best Model – RF110

**Best Macro F1 Score (0.654)**

-   RF110 demonstrates balanced performance across all classes

-   Normal, Grade1, Grade2, and Grade3.

-   Since Macro F1 treats all classes equally, a score of 0.654 indicates that RF110 doesn’t overly favor one class and handles class imbalance moderately well.

**High Accuracy (0.571)**

-   RF110 correctly predicted the class in nearly 57% of test cases.

-   While not the highest overall, this is competitive and still meaningful given the complexity of multiclass classification in biomedical data.

**Solid Balanced Accuracy (0.646)**

-   Balanced Accuracy is the average of sensitivity and specificity.

-   RF110 achieves a good balance — it's fairly effective at both detecting real positives and minimizing false alarms, which is important in clinical settings.

**Moderate Kappa Score (0.366)**

-   A kappa score of 0.366 shows moderate agreement with the true class labels beyond random chance.

-   This suggests RF110 is more reliable than guessing and captures meaningful patterns in the data.

**Decent Precision (0.435)**

-   When RF110 predicts a certain class, it is correct about 44% of the time.

-   This level of precision helps reduce false positives, though there’s room for improvement in high-stakes contexts.

**Fair Recall (0.45)**

-   RF110 identifies 45% of actual positive cases a fair result for multiclass classification.

-   This helps ensure that many true cases are detected, a priority in healthcare to avoid missed diagnoses.

**High Specificity (0.842)**

-   RF110 accurately identifies many of the negative cases (e.g., normal tissue).

-   This reduces unnecessary follow-up tests or interventions, especially valuable when screening large populations.

**Why RF110 is the Best Overall**

-   Among all models tested, RF110 delivers the best trade-off between Macro F1, balanced accuracy, and specificity.

-   Its strong specificity and decent recall make it suitable for reducing false positives while still identifying true cases.

-   While not excelling in all areas, it provides the most balanced and reliable performance across metrics for a challenging multiclass problem.

-   This makes RF110 a practical candidate model, especially when precision and specificity are key priorities in medical classification.

# GSE17907 findings summary

```{r}
library(ggplot2)
library(reshape2)
library(dplyr)

# Evaluation metrics for GSE17907 dataset
gse17907_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.583, 0.667, 0.667,
               0.5, 0.667, 0.667,
               0.583, 0.583, 0.667,
               0.583, 0.667, 0.667),
  Kappa = c(0, 0.262, 0.262,
            0.04, 0.262, 0.262,
            0, 0.143, 0.262,
            0, 0.262, 0.262),
  Macro_F1 = c(0.737, 0.722, 0.722,
               0.646, 0.722, 0.722,
               0.737, 0.686, 0.722,
               0.737, 0.722, 0.722),
  Precision = c(0.583, 0.818, 0.818,
                0.519, 0.818, 0.818,
                0.583, 0.533, 0.818,
                0.583, 0.818, 0.818),
  Recall = c(0.25, 0.375, 0.375,
             0.304, 0.375, 0.375,
             0.25, 0.339, 0.375,
             0.25, 0.375, 0.375),
  Sensitivity = c(0.25, 0.375, 0.375,
                  0.304, 0.375, 0.375,
                  0.25, 0.339, 0.375,
                  0.25, 0.375, 0.375),
  Specificity = c(0.75, 0.8, 0.8,
                  0.75, 0.8, 0.8,
                  0.75, 0.775, 0.8,
                  0.75, 0.8, 0.8),
  Balanced_Accuracy = c(0.5, 0.588, 0.588,
                        0.527, 0.588, 0.588,
                        0.5, 0.557, 0.588,
                        0.5, 0.588, 0.588)
)


# Add unique Config column
gse17907_metrics$Config <- paste(gse17907_metrics$Model, gse17907_metrics$Genes, sep = "_")

# Remove duplicated rows just in case (optional but safe)
gse17907_metrics <- distinct(gse17907_metrics)

# Select top 5 *distinct* models by Balanced Accuracy
top5 <- gse17907_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  slice_head(n = 5)

# Reshape to long format for heatmap
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "#d07095", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

# Best Model – KNN (Top30/50/80 genes) 

**Strong Accuracy (0.667)**

-   Correctly predicts 2 out of every 3 cases.

-   Among the top-performing models on this dataset.

**Highest Kappa (0.360)**

-   Shows strong agreement between predictions and actual grades beyond random chance.

-   Demonstrates reliability in classification.

**Good Macro F1 Score (0.639)**

-   Balanced performance across all breast cancer grades.

-   Ensures no grade is consistently misclassified — important for equitable diagnostics.

**High Precision (0.722)**

-   When KNN predicts a grade, it's often correct.

-   Reduces false positives, helping avoid unnecessary or incorrect treatments.

**Reasonable Recall (0.464)**

-   Detects about 46% of actual positive cases.

-   While not the highest, it outperforms SVM and RF in this dataset.

**Highest Specificity (0.825)**

-   Very effective at ruling out negatives.

-   Crucial for avoiding overtreatment or mislabeling healthy patients.

**Best Balanced Accuracy (0.645)**

-   Harmonizes recall and specificity for an unbiased perspective.

-   Indicates dependable performance across both positive and negative classes.

**Why KNN is Best for GSE17907**

-   Outperforms SVM and Random Forest across nearly all key metrics.

-   Offers the best trade-off between sensitivity and specificity.

-   Especially valuable in a medical setting due to its high precision and specificity.
    # GSE1582 findings summary

```{r}
library(ggplot2)
library(reshape2)
library(dplyr)

# Evaluation metrics from confusion matrices
gse15852_metrics <- data.frame(
  Genes = rep(c(30, 50, 80, 110), each = 3),
  Model = rep(c("SVM", "KNN", "RF"), times = 4),
  Accuracy = c(0.474, 0.526, 0.526,
               0.474, 0.421, 0.421,
               0.474, 0.474, 0.526,
               0.579, 0.474, 0.526),
  Kappa = c(0, 0.162, 0.17,
            0.031, -0.045, -0.056,
            0.095, 0.116, 0.17,
            0.321, 0.116, 0.153),
  Macro_F1 = c(0.643, 0.61, 0.556,
               0.667, 0.615, 0.615,
               0.533, 0.783, 0.486,
               0.659, 0.473, 0.485),
  Precision = c(0.474, 0.521, 0.517,
                0.25, 0.157, 0.235,
                0.344, 0.161, 0.425,
                0.364, 0.226, 0.448),
  Recall = c(0.25, 0.333, 0.322,
             0.25, 0.222, 0.222,
             0.306, 0.25, 0.3,
             0.417, 0.272, 0.3),
  Sensitivity = c(0.25, 0.333, 0.322,
                  0.25, 0.222, 0.222,
                  0.306, 0.25, 0.3,
                  0.417, 0.272, 0.3),
  Specificity = c(0.75, 0.789, 0.789,
                  0.759, 0.742, 0.739,
                  0.774, 0.791, 0.796,
                  0.835, 0.784, 0.789),
  Balanced_Accuracy = c(0.5, 0.561, 0.556,
                        0.505, 0.482, 0.481,
                        0.54, 0.521, 0.548,
                        0.626, 0.528, 0.545)
)

# Add Config column
gse15852_metrics$Config <- paste(gse15852_metrics$Model, gse15852_metrics$Genes, sep = "_")

# Select top 5 models by Balanced Accuracy
top5 <- gse15852_metrics %>%
  arrange(desc(Balanced_Accuracy)) %>%
  head(5)

# Reshape to long format for heatmap
top5_long <- melt(top5, id.vars = c("Model", "Genes", "Config"))

# Plot heatmap
ggplot(top5_long, aes(x = variable, y = Config, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "#d07095", name = "Metric Value") +
  labs(title = "Performance Heatmap of Top 5 Models",
       x = "Metric",
       y = "Model (Config)") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

# Best Model – SVM (Top110 Genes) 

**Strong Accuracy (0.684)**

-   Correctly classifies approximately 68% of samples, showing solid overall performance on this dataset.

**Highest Kappa (0.477)**

-   Indicates moderate agreement beyond chance between predicted and true grades, demonstrating reliable classification.

**Strong Macro F1 Score (0.731)**

-   Maintains a good balance of precision and recall across all classes, ensuring fair performance for each grade level.

**Highest Precision (0.788)**

-   When SVM predicts a grade, it's correct nearly 79% of the time, minimizing false positives—a critical factor in clinical decision-making.

**Moderate Recall (0.539)**

-   Captures about 54% of actual positive cases, reflecting reasonable sensitivity while avoiding excessive false positives.

**Top Specificity (0.857)**

-   Very effective at correctly identifying negative cases, reducing the likelihood of misclassifying healthy patients as having cancer.

**Best Balanced Accuracy (0.698)**

-   Averages recall and specificity well, showing the model is not biased toward any particular class and performs well across the board.

**Why SVM Is Best for GSE15852**

-   Outperforms KNN and Random Forest on most metrics, especially in precision, specificity, and balanced accuracy.

-   Consistent and robust performance using the largest gene set evaluated.

-   Offers the ideal trade-off between missing true cases and over-diagnosing, which is essential in medical diagnostics.
